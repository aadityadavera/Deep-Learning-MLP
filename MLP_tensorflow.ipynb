{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP-tensorflow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "v0hRDnJGY3bH"
      },
      "outputs": [],
      "source": [
        "from numpy.core.arrayprint import format_float_scientific\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "seed = 1234\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "dropout = False\n",
        "L2 = False\n",
        "FMINST = False\n",
        "size_hidden1 = 1024\n",
        "size_hidden2 = 512\n",
        "regularizer_rate = 0.01\n",
        "learning_rate = .01\n",
        "drop_percent = 0.2\n",
        "BATCH_SIZE = 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "  if FMINST:\n",
        "    (X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "  else:\n",
        "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "  X_train = X_train.astype(float) / 255.\n",
        "  X_test = X_test.astype(float) / 255.\n",
        "\n",
        "  X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
        "  X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n",
        "\n",
        "  X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
        "  y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "\n",
        "  ## Changing labels to one-hot encoded vector\n",
        "  lb = LabelBinarizer()\n",
        "  y_train = lb.fit_transform(y_train)\n",
        "  y_test = lb.transform(y_test)\n",
        "  y_val = lb.transform(y_val)\n",
        "\n",
        "  return X_train, X_val, X_test, y_train, y_val, y_test\n"
      ],
      "metadata": {
        "id": "V4hnimWpljcR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test = load_dataset()\n",
        "size_input = X_train.shape[1]\n",
        "size_output = y_train.shape[1]\n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]"
      ],
      "metadata": {
        "id": "6dv_IcvjZJz6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(tf.keras.Model):\n",
        "  def __init__(self, size_input, size_hidden1, size_hidden2, size_output, device=None):\n",
        "    super(MLP, self).__init__()\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    #Hyper parameters: Number of nodes in layer 1, layer 2, learning rate, activation function\n",
        "    # self.size_input = 32\n",
        "    # self.size_hidden = 128\n",
        "    # self.size_output = 1\n",
        "    # self.device = 'gpu'\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2,size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
        "\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output]))\n",
        "    self.b3 = tf.Variable(tf.random.normal([1,self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.MLP_variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #cross entropy add l1, l2\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    if L2:\n",
        "      regularizers = tf.nn.l2_loss(self.W1) + tf.nn.l2_loss(self.W2)\n",
        "      loss = tf.reduce_mean(loss + regularizer_rate * regularizers)\n",
        "    return loss\n",
        "  \n",
        "  def accuracy_score(self, y_pred, y_true):\n",
        "    y_pred = tf.nn.softmax(y_pred)\n",
        "    correct_prediction = tf.equal(tf.argmax(y_true,1), tf.argmax(y_pred,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate= learning_rate)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #print(predicted)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "\n",
        "    grads = tape.gradient(current_loss, self.MLP_variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.MLP_variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    if dropout:\n",
        "      hhat1 = tf.nn.dropout(hhat1,drop_percent)\n",
        "    # Compute output\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    if dropout:\n",
        "      hhat2 = tf.nn.dropout(hhat2,drop_percent)\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "DpGETHIzZj1T"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4mZN3wbIoKU",
        "outputId": "438c209e-e948-47ae-fcf4-1687aae44788"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10\n",
        "# Initialize model using GPU\n",
        "#mlp_on_gpu = MLP()\n",
        "mlp_on_gpu = MLP(size_input, size_hidden1, size_hidden2, size_output)\n",
        "time_start = time.time()\n",
        "training_accuracy = []\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(BATCH_SIZE)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs) \n",
        "  preds = mlp_on_gpu.forward(X_train)\n",
        "  train_accuracy = mlp_on_gpu.accuracy_score(preds,y_train)\n",
        "  print('Number of Epoch = {} - loss:= {:.4f} - Training Accuracy = {:.4f}'.format(epoch + 1, loss_total.numpy() / X_train.shape[0], train_accuracy.numpy()*100))\n",
        "  val_pred = mlp_on_gpu.compute_output(X_val)\n",
        "  val_accuracy = mlp_on_gpu.accuracy_score(val_pred,y_val)\n",
        "  val_accuracy = val_accuracy * 100\n",
        "  print (\"Validation Accuracy = {:.4f}\".format(val_accuracy.numpy()))\n",
        "\n",
        "  test_pred = mlp_on_gpu.compute_output(X_test)\n",
        "  test_accuracy = mlp_on_gpu.accuracy_score(test_pred,y_test)\n",
        "  test_accuracy = test_accuracy*100\n",
        "  print (\"Test Accuracy = {:.4f}\".format(test_accuracy.numpy()))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwg_5g_pZ56Z",
        "outputId": "6954f2f9-baea-4ffa-d497-19c9a269141a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - loss:= 2.0234 - Training Accuracy = 92.7100\n",
            "Validation Accuracy = 91.5100\n",
            "Test Accuracy = 91.0600\n",
            "Number of Epoch = 2 - loss:= 0.1710 - Training Accuracy = 94.9240\n",
            "Validation Accuracy = 92.4600\n",
            "Test Accuracy = 92.1600\n",
            "Number of Epoch = 3 - loss:= 0.0856 - Training Accuracy = 96.3000\n",
            "Validation Accuracy = 92.9400\n",
            "Test Accuracy = 92.7800\n",
            "Number of Epoch = 4 - loss:= 0.0500 - Training Accuracy = 96.8980\n",
            "Validation Accuracy = 92.9500\n",
            "Test Accuracy = 92.5300\n",
            "Number of Epoch = 5 - loss:= 0.0317 - Training Accuracy = 97.3340\n",
            "Validation Accuracy = 93.0800\n",
            "Test Accuracy = 92.7600\n",
            "Number of Epoch = 6 - loss:= 0.0194 - Training Accuracy = 98.0620\n",
            "Validation Accuracy = 93.0900\n",
            "Test Accuracy = 92.9900\n"
          ]
        }
      ]
    }
  ]
}